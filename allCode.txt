# --- Content from C:/research/SemanticNormalForms\calibrate_thresholds.py ---
# calibrate_thresholds.py
"""
Calibration job to compute adaptive thresholds from historical data.

Usage:
    python calibrate_thresholds.py

This script is intentionally simple: it loads sample metrics from a
small local store (or synthetic generator) and updates sdnf_config.json.
In production, replace `collect_samples()` with real telemetry queries.
"""

import numpy as np
from sdnf_config import load_config, save_config
import json
import os

# Replace this with real telemetry collection in production
def collect_samples():
    # Synthetic example: distributions for each metric
    rng = np.random.default_rng(123)
    samples = {
        "EENF": np.abs(rng.normal(0.001, 0.0005, size=1000)).tolist(),
        "AANF": np.clip(rng.normal(0.5, 0.1, size=1000), 0, 1).tolist(),
        "CMNF": np.abs(rng.normal(0.03, 0.01, size=1000)).tolist(),
        "DBNF": np.abs(rng.normal(0.12, 0.08, size=1000)).tolist(),
        "PONF": np.abs(rng.normal(0.02, 0.01, size=1000)).tolist()
    }
    return samples

def calibrate():
    cfg = load_config()
    samples = collect_samples()
    for key in ["EENF", "AANF", "CMNF", "DBNF", "PONF"]:
        if key in samples:
            q = cfg.get(key, {}).get("calibration_quantile", 0.95)
            val = float(np.quantile(samples[key], q))
            # smoothing: blend with existing
            prev = cfg.get(key, {}).get("tau", None)
            if prev is None:
                new_tau = val
            else:
                alpha = 0.3
                new_tau = alpha * val + (1 - alpha) * prev
            cfg.setdefault(key, {})["tau"] = float(new_tau)
            print(f"Calibrated {key}: quantile({q})={val:.6f} -> tau={new_tau:.6f}")
    save_config(cfg)
    print("Saved updated config to sdnf_config.json")

if __name__ == "__main__":
    calibrate()


# --- Content from C:/research/SemanticNormalForms\cmnf.py ---
# cmnf.py
"""
Context Modulation helpers.

Provides learn_linear_projection(X, k, orthogonalize_iters, other_W) which returns a projection matrix W (k x d).
Iterative orthogonalization reduces contamination between contexts.
"""

import numpy as np

def learn_linear_projection(X: np.ndarray, k: int = None, orthogonalize_iters: int = 0, other_W: np.ndarray = None):
    X = np.asarray(X)
    if X.ndim != 2:
        raise ValueError("X must be 2D (n x d)")
    n, d = X.shape
    if k is None:
        k = min(64, d)
    Xc = X - np.mean(X, axis=0, keepdims=True)
    try:
        U, S, Vt = np.linalg.svd(Xc, full_matrices=False)
        W = Vt[:k, :].astype(np.float32)
    except Exception:
        rng = np.random.default_rng(0)
        A = rng.normal(size=(k, d))
        Q, R = np.linalg.qr(A.T)
        W = Q.T[:k, :].astype(np.float32)

    # iterative orthogonalization relative to other_W
    if other_W is not None and orthogonalize_iters > 0:
        OW = np.asarray(other_W, dtype=np.float32)
        for _ in range(orthogonalize_iters):
            try:
                P = OW.T @ np.linalg.pinv(OW @ OW.T)
                for i in range(W.shape[0]):
                    w = W[i:i+1, :].T
                    proj = P @ (OW @ w)
                    w_new = w - proj
                    W[i:i+1, :] = (w_new.T)
                # re-orthonormalize rows
                U2, S2, Vt2 = np.linalg.svd(W, full_matrices=False)
                W = Vt2[:W.shape[0], :].astype(np.float32)
            except Exception:
                break
    return W.astype(np.float32)


# --- Content from C:/research/SemanticNormalForms\demo.py ---
# demo.py
"""
Demo runner with automatic calibration step.

This file performs two phases:
1) Calibration phase (preceding step)
   - Programmatically derives recommended thresholds for EENF, DBNF, CMNF
     from the JSON files in ./data and from simulated model dynamics.
   - Produces a short calibration report and a suggested config dictionary.
   - Comments throughout explain the rationale and how to adapt the procedure
     as data and operational requirements evolve.

2) Demo phase (uses calibrated thresholds)
   - Runs the SDNF demo (semantic compliance, drift predeployment, evidence accumulation)
   - Uses the calibrated thresholds in-memory for validator and decision policy.
   - Prints the same tables as before, but now thresholds are data-driven.

Notes for implementers:
- Calibration is intentionally conservative: thresholds are set to high quantiles
  (default 95th) of observed statistics to control false positives.
- Calibration is *not* a substitute for governance: borderline cases should still
  be routed to human review (merge_pending) and canary promotion.
- You can persist the suggested config to sdnf_config.json if you want to make
  the calibration permanent; this script only uses the values in-memory.
"""

import json
import logging
import math
import os
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
from tabulate import tabulate

from srs_store import SRSStore
from embedding_utils import EmbeddingModel
from schema_ingest import load_json_file
from cmnf import learn_linear_projection
from sdnf_config import load_config
from semantic_merge import evidence_score

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("sdnf.demo")
np.random.seed(42)


# ---------------------------
# Calibration utilities
# ---------------------------

def list_data_files(data_dir: str = "data") -> List[Path]:
    """
    Return a list of JSON files in the data directory that we will use for calibration.
    Implementers: add or remove files here to reflect your production corpus.
    """
    p = Path(data_dir)
    if not p.exists():
        return []
    return sorted([f for f in p.glob("*.json")])


def load_attribute_names_from_file(path: str) -> List[str]:
    """
    Load attribute names from a schema JSON file if it contains an 'attributes' list.
    Fallback: if the file is a flat payload, return top-level keys.
    """
    data = load_json_file(path)
    if not data:
        return []
    if isinstance(data, dict) and "attributes" in data:
        return [a.get("name") for a in data.get("attributes", []) if a.get("name")]
    # fallback: top-level keys (payload-style files)
    if isinstance(data, dict):
        return [k for k in data.keys()]
    return []


def calibrate_thresholds_from_data(model: EmbeddingModel,
                                   data_dir: str = "data",
                                   eenf_regenerations: int = 20,
                                   drift_noise_scales: List[float] = None,
                                   cmnf_bootstrap_iters: int = 50,
                                   quantile: float = 0.95) -> Tuple[Dict, Dict]:
    """
    Programmatically derive calibration statistics and suggested thresholds.

    Steps and rationale (high level):
    - EENF: For each canonical attribute (we use INAmex.json as canonical SRS),
      we compute G regenerations (deterministic but varied via regen_idx) and
      measure per-dimension variance. The mean per-dimension variance per attribute
      is the EENF statistic. We set tau to the chosen quantile across attributes.
      Rationale: EENF should tolerate typical embedding jitter but flag outliers.

    - DBNF: We compute canonical centroids for each attribute (centroid of name + aliases).
      We simulate plausible drift by adding Gaussian noise at several scales and
      measure L2 drift per attribute. We aggregate drifts across scales and set
      global DBNF tau to the chosen quantile of the simulated drift distribution.
      Rationale: Drift arises from model updates or data shifts; simulation gives
      a conservative estimate of expected drift.

    - CMNF: Learn projection matrices from two distinct context corpora (Stripe.json
      for payments and INAmex.json for risk in the demo). Compute contamination as
      the mean normalized singular value of M = Wp @ Wr^T. Bootstrap small perturbations
      to estimate variability and set tau to the chosen quantile.
      Rationale: CMNF measures subspace overlap; we calibrate to observed overlap.

    Returns:
      (report, suggested_config)
      - report: detailed numeric statistics for inspection
      - suggested_config: recommended tau values and multipliers (in-memory)
    """
    if drift_noise_scales is None:
        drift_noise_scales = [0.01, 0.02, 0.03, 0.05]

    files = list_data_files(data_dir)
    report = {"files_used": [str(f) for f in files], "EENF": {}, "DBNF": {}, "CMNF": {}}

    # Load canonical SRS (prefer INAmex.json if present)
    inamex_path = Path(data_dir) / "INAmex.json"
    if not inamex_path.exists():
        # fallback to first schema-like file
        inamex_path = files[0] if files else None

    # Build canonical attribute list
    canonical_attrs = []
    if inamex_path:
        canonical_attrs = load_attribute_names_from_file(str(inamex_path))
    canonical_attrs = canonical_attrs or []

    # EENF: regenerations per canonical attribute
    eenf_vals = []
    for name in canonical_attrs:
        regs = model.regenerations(name, context="payments", G=eenf_regenerations)
        per_dim_var = np.var(regs, axis=0)
        mean_var = float(np.mean(per_dim_var))
        report["EENF"][name] = {"mean_per_dim_var": mean_var}
        eenf_vals.append(mean_var)
    eenf_vals = np.array(eenf_vals) if eenf_vals else np.array([0.0])
    eenf_q = float(np.quantile(eenf_vals, quantile))
    report["EENF_summary"] = {"median": float(np.median(eenf_vals)), "quantile": eenf_q, "count": len(eenf_vals)}

    # DBNF: canonical centroids and simulated drift
    canonical_centroids = {}
    if inamex_path:
        inam = load_json_file(str(inamex_path)) or {}
        for a in inam.get("attributes", []):
            name = a.get("name")
            tokens = [name] + a.get("aliases", [])
            if not tokens:
                continue
            embs = model.encode(tokens, context="payments")
            centroid = np.mean(embs, axis=0)
            centroid = centroid / (np.linalg.norm(centroid) + 1e-12)
            canonical_centroids[name] = centroid

    dbnf_drifts_all = []
    dbnf_per_attr = {}
    for name, vec in canonical_centroids.items():
        drifts = []
        for s in drift_noise_scales:
            noise = np.random.default_rng(0).normal(scale=s, size=vec.shape)
            pert = vec + noise
            pert = pert / (np.linalg.norm(pert) + 1e-12)
            drift = float(np.linalg.norm(pert - vec))
            drifts.append(drift)
            dbnf_drifts_all.append(drift)
        dbnf_per_attr[name] = {"drifts": drifts, "mean": float(np.mean(drifts))}
    dbnf_drifts_all = np.array(dbnf_drifts_all) if dbnf_drifts_all else np.array([0.0])
    dbnf_q = float(np.quantile(dbnf_drifts_all, quantile))
    report["DBNF_summary"] = {"global_median": float(np.median(dbnf_drifts_all)), "quantile": dbnf_q, "per_attribute": dbnf_per_attr}

    # CMNF: learn W_p from Stripe.json and W_r from INAmex.json (contexts)
    stripe_attrs = load_attribute_names_from_file(str(Path(data_dir) / "Stripe.json"))
    inamex_attrs = canonical_attrs  # reuse INAmex attributes for the other context

    # If Stripe has no attributes, fallback to using canonical centroids as small corpus
    if stripe_attrs:
        stripe_embs = model.encode([t for t in stripe_attrs], context="payments")
    else:
        stripe_embs = np.stack(list(canonical_centroids.values())) if canonical_centroids else np.zeros((1, model.dim))

    if inamex_attrs:
        inamex_embs = model.encode([t for t in inamex_attrs], context="risk")
    else:
        inamex_embs = np.stack(list(canonical_centroids.values())) if canonical_centroids else np.zeros((1, model.dim))

    # Learn projections (SVD-based)
    k_p = min(32, stripe_embs.shape[1])
    k_r = min(32, inamex_embs.shape[1])
    W_p = learn_linear_projection(stripe_embs, k=min(32, stripe_embs.shape[1]))
    W_r = learn_linear_projection(inamex_embs, k=min(32, inamex_embs.shape[1]))

    # contamination metric: mean normalized singular values of M = Wp @ Wr.T
    try:
        M = np.matmul(W_p, W_r.T)
        sv = np.linalg.svd(M, compute_uv=False)
        if sv.size > 0:
            max_sv = float(np.max(np.abs(sv)))
            contamination = float(np.mean(np.abs(sv) / (max_sv if max_sv > 0 else 1.0)))
        else:
            contamination = 0.0
    except Exception:
        contamination = 0.0

    # bootstrap small perturbations to estimate variability
    conts = []
    rng = np.random.default_rng(1)
    for i in range(cmnf_bootstrap_iters):
        sp = stripe_embs + rng.normal(scale=1e-3, size=stripe_embs.shape)
        ir = inamex_embs + rng.normal(scale=1e-3, size=inamex_embs.shape)
        Wp = learn_linear_projection(sp, k=min(32, sp.shape[1]))
        Wr = learn_linear_projection(ir, k=min(32, ir.shape[1]))
        M2 = np.matmul(Wp, Wr.T)
        sv2 = np.linalg.svd(M2, compute_uv=False)
        if sv2.size > 0:
            max_sv2 = float(np.max(np.abs(sv2)))
            conts.append(float(np.mean(np.abs(sv2) / (max_sv2 if max_sv2 > 0 else 1.0))))
        else:
            conts.append(0.0)
    conts = np.array(conts) if conts else np.array([0.0])
    cmnf_q = float(np.quantile(conts, quantile))
    report["CMNF_summary"] = {"median": float(np.median(conts)), "quantile": cmnf_q, "observed": float(contamination)}

    # Suggest config values based on quantiles
    suggested = {
        "EENF": {"tau": eenf_q, "note": f"{int(quantile*100)}th quantile of per-attribute mean variance"},
        "DBNF": {"global_tau": dbnf_q, "adaptive_multiplier": None, "note": f"{int(quantile*100)}th quantile of simulated per-attribute drifts"},
        "CMNF": {"tau": cmnf_q, "note": f"{int(quantile*100)}th quantile of bootstrap contamination"}
    }

    # Suggest adaptive_multiplier: choose multiplier so that multiplier*sqrt(mean variance) ~ DBNF global tau
    mean_var_mean = float(np.mean(eenf_vals)) if eenf_vals.size > 0 else 0.0
    if mean_var_mean > 0:
        suggested_multiplier = suggested["DBNF"]["global_tau"] / math.sqrt(mean_var_mean)
        # clamp to reasonable range
        suggested_multiplier = float(max(1.0, min(suggested_multiplier, 10.0)))
    else:
        suggested_multiplier = 2.5
    suggested["DBNF"]["adaptive_multiplier"] = suggested_multiplier

    return report, suggested


# ---------------------------
# Demo run (uses calibrated thresholds)
# ---------------------------

def run_demo_and_tables_with_calibration():
    """
    1) Run calibration
    2) Use suggested thresholds to override config in-memory
    3) Run the demo using the calibrated thresholds
    """
    print("\n[SDNF DEMO RUNNER] Starting calibration phase...\n")
    cfg = load_config()
    model = EmbeddingModel()
    # calibrate using local data folder
    report, suggested = calibrate_thresholds_from_data(model, data_dir="data", eenf_regenerations=20,
                                                       drift_noise_scales=[0.01, 0.02, 0.03, 0.05],
                                                       cmnf_bootstrap_iters=50, quantile=0.95)

    # Print calibration report (concise)
    print("CALIBRATION REPORT (concise):")
    # EENF summary
    eenf_tau = suggested["EENF"]["tau"]
    dbnf_tau = suggested["DBNF"]["global_tau"]
    cmnf_tau = suggested["CMNF"]["tau"]
    adaptive_multiplier = suggested["DBNF"]["adaptive_multiplier"]

    table = [
        ["EENF (mean per-dim var) 95th quantile", f"{eenf_tau:.6f}"],
        ["DBNF (simulated drift) 95th quantile", f"{dbnf_tau:.6f}"],
        ["CMNF (contamination) 95th quantile", f"{cmnf_tau:.6f}"],
        ["DBNF adaptive_multiplier (suggested)", f"{adaptive_multiplier:.3f}"]
    ]
    print(tabulate(table, headers=["Metric", "Suggested value"], tablefmt="grid"))

    print("\nCalibration notes:")
    print("- EENF: uses regenerations per attribute to estimate embedding jitter.")
    print("- DBNF: simulated drift across multiple plausible noise scales; conservative quantile chosen.")
    print("- CMNF: bootstrap contamination across small perturbations; increase orthogonalize_iters if you want stricter separation.\n")

    # Apply suggested thresholds in-memory (do not overwrite file)
    # We update the loaded config dict so the rest of the demo uses calibrated values.
    cfg_local = cfg.copy()
    cfg_local["EENF"] = cfg_local.get("EENF", {})
    cfg_local["EENF"]["tau"] = eenf_tau
    cfg_local["DBNF"] = cfg_local.get("DBNF", {})
    cfg_local["DBNF"]["global_tau"] = dbnf_tau
    cfg_local["DBNF"]["adaptive_multiplier"] = adaptive_multiplier
    cfg_local["CMNF"] = cfg_local.get("CMNF", {})
    cfg_local["CMNF"]["tau"] = cmnf_tau

    # Inform implementers: you can persist cfg_local to sdnf_config.json if desired.
    print("Using calibrated thresholds for this demo run (in-memory). To persist, write cfg_local to sdnf_config.json.\n")

    # Proceed to run the demo using SRSStore and the in-memory cfg_local values.
    # We will instantiate SRSStore and override its config where needed.
    store = SRSStore(embedding_model=model)
    # override store.config values used by validator and merger
    store.config.update(cfg_local)
    # reinitialize merger and validator with new thresholds
    # (Simpler approach: update validator.tau and merger.tau_dbnf directly)
    try:
        store.validator.tau["EENF"] = cfg_local["EENF"]["tau"]
        store.validator.tau["CMNF"] = cfg_local["CMNF"]["tau"]
        store.validator.tau["DBNF"] = cfg_local["DBNF"]["global_tau"]
    except Exception:
        pass
    try:
        store.merger.tau_dbnf = cfg_local["DBNF"]["global_tau"]
    except Exception:
        pass

    # Load base master SRS (INAmex.json) if available
    base = load_json_file("INAmex.json")
    if base:
        store.master = base
        store._recompute_all_canonical_embeddings()

    # --- The rest of the demo is similar to previous demo.py but uses calibrated thresholds ---
    print("MASTER SRS (initial):")
    master = store.get_master_schema()
    if master.get("attributes"):
        rows = [[a["name"], a.get("type", ""), ",".join(a.get("aliases", []))] for a in master["attributes"]]
        print(tabulate(rows, headers=["name", "type", "aliases"], tablefmt="grid"))
    else:
        print("(master SRS empty)")

    # Use Case 1: Semantic compliance
    payload = {"payer_vpa": "user@upi", "txn_amount": "1000", "merchant_id": "MID123"}
    compliance_report = store.check_payload_compliance(payload, context="payments")
    uc1_pass = isinstance(compliance_report, dict) and "mapped_fields" in compliance_report and "sdnf_status" in compliance_report
    uc1_notes = "unmapped_fields:" + ",".join([k for k, v in compliance_report["mapped_fields"].items() if v is None]) if uc1_pass else "report_missing"

    # Use Case 2: EENF + DBNF drift simulation (use calibrated DBNF tau)
    pre_embeddings_map = dict(store._canonical_embeddings)
    rng = np.random.default_rng(1234)
    post_embeddings_map = {}
    noise = rng.normal(scale=cfg_local.get("random", {}).get("drift_noise_scale", 0.02), size=(1, store.model.dim))
    for name, vec in pre_embeddings_map.items():
        # small perturbation to simulate drift
        noise = rng.normal(scale=0.02, size=vec.shape)
        perturbed = vec + noise
        if np.linalg.norm(perturbed) > 0:
            perturbed = perturbed / (np.linalg.norm(perturbed) + 1e-12)
        post_embeddings_map[name] = perturbed

    per_attribute_tau = store.compute_adaptive_dbnf_taus()
    # ensure adaptive multiplier from calibration is used
    store.config["DBNF"]["adaptive_multiplier"] = cfg_local["DBNF"]["adaptive_multiplier"]
    global_tau = cfg_local["DBNF"]["global_tau"]
    # compute per-attribute DBNF details
    def compute_per_attribute_dbnf(pre_map, post_map, per_attribute_tau, global_tau):
        details = []
        sample_vec = None
        if pre_map:
            sample_vec = next(iter(pre_map.values()))
        elif post_map:
            sample_vec = next(iter(post_map.values()))
        else:
            return details
        zero = np.zeros_like(sample_vec)
        all_attrs = sorted(set(list(pre_map.keys()) + list(post_map.keys())))
        for a in all_attrs:
            pre = pre_map.get(a, None)
            post = post_map.get(a, None)
            pvec = pre if pre is not None else zero
            qvec = post if post is not None else zero
            if np.linalg.norm(pvec) > 0:
                pvec = pvec / (np.linalg.norm(pvec) + 1e-12)
            if np.linalg.norm(qvec) > 0:
                qvec = qvec / (np.linalg.norm(qvec) + 1e-12)
            drift = float(np.linalg.norm(qvec - pvec))
            tau_attr = float(per_attribute_tau.get(a, global_tau))
            ok = drift <= tau_attr
            action = "accept" if ok else "fork"
            details.append({"attribute": a, "drift": drift, "pass": ok, "action": action, "tau": tau_attr})
        return details

    attr_dbnf_details = compute_per_attribute_dbnf(pre_embeddings_map, post_embeddings_map, per_attribute_tau, global_tau)

    # Use Case 3: evidence accumulation and merges (per-merge ECNF)
    payloads = [
        {"acct_num": "A1", "amount": "10"},
        {"acct_num": "A2", "amount": "20"},
        {"acct_num": "A3", "amount": "30"},
        {"acct_num": "acct_num", "amount": "40"}
    ]
    before_lineage = len(store.list_lineage(1000))
    rec_ids = []
    per_merge_reports = []
    for i, p in enumerate(payloads):
        r = store.insert_payload(p, source=f"sim_source_{i}", auto_merge=True)
        rec_ids.append(r.get("record_id"))
        new_entries = store.list_lineage(1000)[before_lineage:]
        for e in new_entries:
            if e.get("action", "").startswith("merge") or e.get("action") == "merge_pending":
                per_merge_reports.append(e)
        before_lineage = len(store.list_lineage(1000))

    after_lineage = len(store.list_lineage(1000))
    recent = store.list_lineage(50)
    merge_actions = [e for e in recent if e["action"].startswith("merge") or e["action"] == "merge_pending"]
    uc3_pass = len(merge_actions) > 0
    uc3_notes = f"lineage_added={len(per_merge_reports)}; merge_actions={len(merge_actions)}"

    # Table A
    table_a = [
        ["Semantic Compliance Check", "PASS" if uc1_pass else "FAIL", uc1_notes],
        ["Regression Drift Predeployment", "PASS" if all(d["pass"] for d in attr_dbnf_details) else "FAIL", f"failed_attrs={sum(1 for d in attr_dbnf_details if not d['pass'])}/{len(attr_dbnf_details)}"],
        ["Evidence Accumulation Merge", "PASS" if uc3_pass else "FAIL", uc3_notes]
    ]
    print("\nTABLE A: Semantic Use Case Test Results")
    print(tabulate(table_a, headers=["Use Case", "Result", "Notes"], tablefmt="grid"))

    # Table B
    lineage = store.list_lineage(500)
    merge_counts = {}
    example_actions = {}
    for e in lineage:
        action = e.get("action")
        evidence = e.get("evidence", [])
        tokens = [it.get("token") for it in evidence if it.get("token")]
        key = action
        merge_counts[key] = merge_counts.get(key, 0) + 1
        if key not in example_actions and tokens:
            example_actions[key] = tokens[:3]
    table_b = []
    if merge_counts:
        for k, cnt in merge_counts.items():
            ex = ",".join(example_actions.get(k, [])) if example_actions.get(k) else ""
            table_b.append([k, cnt, ex])
    else:
        table_b = [["(no lineage yet)", 0, ""]]
    print("\nTABLE B: Merge Summary Snapshot")
    print(tabulate(table_b, headers=["Action", "Count", "Example Tokens"], tablefmt="grid"))

    # Table C: per-attribute taus (adaptive)
    print("\nTABLE C: Per-attribute adaptive DBNF taus")
    rows_c = []
    for name, tau in sorted(per_attribute_tau.items()):
        rows_c.append([name, f"{tau:.6f}"])
    if rows_c:
        print(tabulate(rows_c, headers=["attribute", "tau_attr"], tablefmt="grid"))
    else:
        print("(no attributes)")

    # Table D: DBNF attribute-level details
    print("\nTABLE D: DBNF attribute-level details")
    rows_d = []
    for d in attr_dbnf_details:
        rows_d.append([d["attribute"], f"{d['drift']:.6f}", "PASS" if d["pass"] else "FAIL", d["action"], f"{d['tau']:.6f}"])
    if rows_d:
        print(tabulate(rows_d, headers=["attribute", "drift", "status", "action", "tau_used"], tablefmt="grid"))
    else:
        print("(no DBNF details)")

    # Master SRS final snapshot
    final_master = store.get_master_schema()
    print("\nMASTER SRS final snapshot:")
    if final_master.get("attributes"):
        rows = [[a["name"], a.get("type", ""), ",".join(a.get("aliases", []))] for a in final_master["attributes"]]
        print(tabulate(rows, headers=["name", "type", "aliases"], tablefmt="grid"))
    else:
        print("(master SRS empty)")

    # CMNF: learn W_p from Stripe.json and W_r from INAmex.json with orthogonalization
    stripe_attrs = load_attribute_names_from_file("Stripe.json")
    inamex_attrs = load_attribute_names_from_file("INAmex.json")
    if not stripe_attrs:
        stripe_attrs = [a["name"] for a in final_master.get("attributes", [])]
    if not inamex_attrs:
        inamex_attrs = [a["name"] for a in final_master.get("attributes", [])]

    stripe_embs = model.encode([t for t in stripe_attrs], context="payments")
    inamex_embs = model.encode([t for t in inamex_attrs], context="risk")

    orth_iters = int(cfg_local.get("CMNF", {}).get("orthogonalize_iters", 3))
    W_p = learn_linear_projection(stripe_embs, k=min(32, stripe_embs.shape[1]))
    W_r = learn_linear_projection(inamex_embs, k=min(32, inamex_embs.shape[1]))

    # compute contamination on contextual components using singular values of cross-projection
    if final_master.get("attributes"):
        sample_embeddings = np.stack([store._canonical_embeddings.get(n, store._embed(n)) for n in [a["name"] for a in final_master["attributes"]]], axis=0)
        d = model.contextual_dim
        contextual_sample = sample_embeddings[:, -d:]
        try:
            Wp_ctx = W_p[:, -d:] if W_p.shape[1] >= d else W_p
            Wr_ctx = W_r[:, -d:] if W_r.shape[1] >= d else W_r
            M = np.matmul(Wp_ctx, Wr_ctx.T)
            sv = np.linalg.svd(M, compute_uv=False)
            contamination = float(np.mean(np.abs(sv))) if sv.size > 0 else 0.0
        except Exception:
            contamination = 0.0
    else:
        contamination = 0.0

    # Table I: SDNF 7-NF compliance (CMNF replaced with contamination)
    attr_names = [a["name"] for a in final_master.get("attributes", [])]
    if attr_names:
        attr_embeddings = np.stack([store._canonical_embeddings.get(n, store._embed(n)) for n in attr_names], axis=0)
        pre_vec = np.mean(np.stack(list(pre_embeddings_map.values()), axis=0), axis=0) if pre_embeddings_map else np.zeros((store.model.dim,))
        post_vec = np.mean(np.stack(list(post_embeddings_map.values()), axis=0), axis=0) if post_embeddings_map else pre_vec
        # reuse existing validator run_all but inject contamination into CMNF result
        sdnf_table = store.validator.run_all({
            "regenerations": attr_embeddings if attr_embeddings.size else np.zeros((1, store.model.dim)),
            "attr_embeddings": attr_embeddings,
            "attr_names": attr_names,
            "W_p": W_p, "W_r": W_r,
            "sample_embeddings": attr_embeddings if attr_embeddings.size else np.zeros((1, store.model.dim)),
            "pre_vec": pre_vec, "post_vec": post_vec,
            "tau_dbnf": store.validator.tau.get("DBNF", cfg_local["DBNF"]["global_tau"]),
            "relations": store.master.get("relations", []),
            "role_compat": {},
            "partition_labels": [0] * len(attr_names),
            # optionally include evidence_summary if you want ECNF to be evaluated here
            "evidence_summary": None
        })
        for r in sdnf_table:
            if r["name"] == "CMNF":
                r["actual"] = contamination
                r["status"] = "PASS" if contamination < store.validator.tau.get("CMNF", cfg_local["CMNF"]["tau"]) else "FAIL"
        print("\nTABLE I: SDNF 7-Normal Form Compliance Summary (calibrated thresholds)")
        rows = []
        for r in sdnf_table:
            actual = r["actual"]
            if isinstance(actual, float):
                actual_fmt = f"{actual:.6f}"
            else:
                actual_fmt = str(actual)
            rows.append([r["name"], r.get("req", ""), actual_fmt, r.get("status", ""), r.get("details", {})])
        print(tabulate(rows, headers=["Normal Form", "Requirement", "Actual", "Status", "Details"], tablefmt="grid"))
    else:
        print("\nNo attributes available to compute full SDNF table.")

    # Lineage excerpt and forks
    lineage = store.list_lineage(500)
    print("\nLINEAGE EXCERPT (last 10 entries):")
    for entry in lineage[-10:]:
        print(entry)

    forks = [e for e in lineage if e["action"] == "fork"]
    if forks:
        print("\nDBNF Fork Events (attribute-level):")
        for f in forks[-10:]:
            print(f)
    else:
        print("\nNo DBNF fork events recorded.")

    # Return calibration report and demo outputs for programmatic inspection if needed
    return {"calibration_report": report, "suggested_config": suggested, "demo_store": store}


if __name__ == "__main__":
    run_demo_and_tables_with_calibration()


# --- Content from C:/research/SemanticNormalForms\embedding_utils.py ---
# embedding_utils.py
"""
Multi-level deterministic embedding model implementing Eq 3.2 with regeneration support.

Emb(p, c) = concat(Emb_fine(p, c), Emb_abstract(p), Emb_contextual(p, c))

Supports deterministic regenerations (simulate repeated embedding calls) for EENF.
"""

from typing import List, Optional, Tuple
import numpy as np
import hashlib

class EmbeddingModel:
    def __init__(self,
                 fine_dim: int = 128,
                 abstract_dim: int = 64,
                 contextual_dim: int = 64,
                 regen_jitter: float = 1e-3):
        self.fine_dim = fine_dim
        self.abstract_dim = abstract_dim
        self.contextual_dim = contextual_dim
        self.dim = fine_dim + abstract_dim + contextual_dim
        self.regen_jitter = regen_jitter

    def _seed_from_text(self, text: str, salt: str = "") -> int:
        h = hashlib.sha256((text + salt).encode("utf-8")).digest()
        return int.from_bytes(h[:8], "little") & 0xFFFFFFFF

    def _pseudo_vector(self, text: str, dim: int, salt: str = "", regen_idx: Optional[int] = None) -> np.ndarray:
        seed = self._seed_from_text(text, salt)
        if regen_idx is not None:
            seed = (seed ^ (regen_idx * 0x9e3779b1)) & 0xFFFFFFFF
        rng = np.random.default_rng(seed)
        v = rng.normal(size=(dim,))
        n = np.linalg.norm(v) + 1e-12
        return (v / n).astype(np.float32)

    def encode(self, tokens: List[str], context: Optional[str] = None, regen_idx: Optional[int] = None) -> np.ndarray:
        """
        Encode tokens into concatenated embeddings. If regen_idx is provided, deterministic
        regeneration variation is applied to simulate repeated embedding calls.
        """
        out = []
        ctx = context or "global"
        for t in tokens:
            fine = self._pseudo_vector(f"LEX:{t}:{ctx}", self.fine_dim, salt="fine", regen_idx=regen_idx)
            abstract = self._pseudo_vector(f"ABS:{t}", self.abstract_dim, salt="abstract", regen_idx=regen_idx)
            contextual = self._pseudo_vector(f"CTX:{ctx}:{t}", self.contextual_dim, salt="contextual", regen_idx=regen_idx)
            emb = np.concatenate([fine, abstract, contextual], axis=0)
            # small deterministic jitter for regeneration realism
            if regen_idx is not None:
                jitter = (regen_idx % 7) * self.regen_jitter
                emb = emb + jitter * np.sign(emb)
            out.append(emb)
        return np.stack(out, axis=0)

    def regenerations(self, token: str, context: Optional[str], G: int) -> np.ndarray:
        """
        Return G regenerations for token as an array (G x dim).
        """
        reps = [self.encode([token], context=context, regen_idx=i)[0] for i in range(G)]
        return np.stack(reps, axis=0)

    def split_components(self, emb: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        fine = emb[:self.fine_dim]
        abstract = emb[self.fine_dim:self.fine_dim + self.abstract_dim]
        contextual = emb[self.fine_dim + self.abstract_dim:]
        return fine, abstract, contextual

    def component_similarity(self, a: np.ndarray, b: np.ndarray) -> dict:
        def cos(u, v):
            nu = np.linalg.norm(u) + 1e-12
            nv = np.linalg.norm(v) + 1e-12
            return float(np.dot(u, v) / (nu * nv))
        fa, aa, ca = self.split_components(a)
        fb, ab, cb = self.split_components(b)
        return {
            "fine": cos(fa, fb),
            "abstract": cos(aa, ab),
            "contextual": cos(ca, cb),
            "global": cos(a, b)
        }


# --- Content from C:/research/SemanticNormalForms\preprocessing.py ---
# preprocessing.py
import re

def preprocess(names, source):
    """
    Standardizes attribute names for the SDNF embedding pipeline.
    """
    results = []
    for name in names:
        norm = name.lower().strip()
        norm = re.sub(r'[^a-z0-9]', '', norm)
        results.append({
            "original": name,
            "normalized": norm,
            "source": source
        })
    return results


# --- Content from C:/research/SemanticNormalForms\schema_ingest.py ---
# schema_ingest.py
import re
import json
import logging
import datetime
import os
from typing import Dict, Any, List

logger = logging.getLogger("sdnf.ingest")


def load_json_file(filename: str):
    """Look for file in given path or in ./data/; return parsed JSON or None."""
    paths = [filename, os.path.join("data", filename)]
    for path in paths:
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
    logger.warning(f"File not found: {filename} (synth will be used by demo)")
    return None


def validate_payload(payload: Dict[str, Any], required_fields: List[str]) -> bool:
    """Simple required-field validator; logs missing fields."""
    missing = [f for f in required_fields if f not in payload]
    if missing:
        logger.error(f"Payload validation failed. Missing fields: {missing}")
        return False
    return True


def infer_type(value):
    """
    Infer a coarse type for a payload value.
    Kept intentionally simple â€” extendable for domain-specific heuristics.
    """
    if isinstance(value, str) and re.match(r"^[0-9\s\-]{13,19}$", value):
        return "pan_type"
    if isinstance(value, int):
        return "integer"
    if isinstance(value, float):
        return "float"
    return "string"


def derive_schema_from_payload(payload: Dict[str, Any], source: str = "unknown") -> Dict[str, Any]:
    """
    Derive a minimal semantic schema object from a single JSON payload.

    Output structure:
    {
      "entity": "DerivedFrom_<source>",
      "attributes": [
         {"name": "<field>", "type": "<inferred>", "aliases": [], "provenance": {"source": source, "first_seen": timestamp}}
      ],
      "derived_at": "<iso timestamp>"
    }
    """
    derived = {
        "entity": f"DerivedFrom_{source}",
        "attributes": [],
        "derived_at": datetime.datetime.utcnow().isoformat() + "Z",
    }
    for k, v in payload.items():
        attribute = {
            "name": str(k),
            "type": infer_type(v),
            "aliases": [],
            "constraints": None,
            "provenance": {"source": source, "first_seen": datetime.datetime.utcnow().isoformat() + "Z"},
        }
        # Small heuristics to populate constraints
        if attribute["type"] == "pan_type":
            attribute["constraints"] = {"pattern": r"^[0-9\s\-]{13,19}$"}
        derived["attributes"].append(attribute)
    return derived


# --- Content from C:/research/SemanticNormalForms\sdnf_config.py ---
# sdnf_config.py
import json
import os
from typing import Dict, Any

CONFIG_PATH = "sdnf_config.json"

DEFAULT_CONFIG = {
    "EENF": {"tau": 0.01, "calibration_quantile": 0.95},
    "AANF": {"tau": 0.90, "calibration_quantile": 0.99},
    "CMNF": {"tau": 0.05, "calibration_quantile": 0.95},
    "DBNF": {"global_tau": 0.25, "calibration_quantile": 0.90, "adaptive_multiplier": 1.5},
    "ECNF": {"m_min": 3, "score_threshold": 0.55, "strong_score_threshold": 0.72},
    "RRNF": {"tau": 0.70},
    "PONF": {"tau": 0.10},
    "decision_policy": {
        "dbnf_fail_fraction": 0.5,
        "dbnf_median_factor": 2.0,
        "auto_merge_min_score": 0.80,
        "auto_merge_min_count": 4
    }
}


def load_config() -> Dict[str, Any]:
    if os.path.exists(CONFIG_PATH):
        try:
            with open(CONFIG_PATH, "r", encoding="utf-8") as f:
                cfg = json.load(f)
            # merge defaults for missing keys
            merged = DEFAULT_CONFIG.copy()
            merged.update(cfg)
            return merged
        except Exception:
            return DEFAULT_CONFIG.copy()
    return DEFAULT_CONFIG.copy()


def save_config(cfg: Dict[str, Any]) -> None:
    with open(CONFIG_PATH, "w", encoding="utf-8") as f:
        json.dump(cfg, f, indent=2)


# --- Content from C:/research/SemanticNormalForms\semantic_merge.py ---
# semantic_merge.py
"""
SemanticMerger with:
- per-merge ECNF evaluation (config-driven)
- canonical selection by highest aggregate evidence score
- DBNF per-attribute tau usage and lineage recording (tau included)
- auto-merge policy parameterized (canary fraction support placeholder)
"""

import logging
import datetime
from typing import Dict, Any, List, Optional, Tuple
import numpy as np
from sdnf_config import load_config

logger = logging.getLogger("sdnf.merge")
cfg = load_config()

def evidence_score(items: List[Dict[str, Any]], weights: Optional[Dict[str, float]] = None) -> float:
    if weights is None:
        weights = {"nn": 0.35, "abstract_nn": 0.20, "ontology": 0.20, "cooccurrence": 0.15, "type_match": 0.06, "heuristic": 0.04}
    total_w = 0.0
    weighted = 0.0
    for it in items or []:
        t = it.get("type", "nn")
        s = float(it.get("score", 0.5))
        w = weights.get(t, 0.0)
        weighted += w * s
        total_w += w
    if total_w <= 0:
        return 0.0
    return max(0.0, min(1.0, weighted / total_w))

def distinct_signal_types(evidence_items: List[Dict[str, Any]]) -> int:
    return len(set(it.get("type") for it in (evidence_items or [])))

class SemanticMerger:
    def __init__(self, m_min: int = None, tau_dbnf: float = None):
        cfg_local = load_config()
        self.m_min = int(m_min or cfg_local["ECNF"]["m_min"])
        self.tau_dbnf = float(tau_dbnf or cfg_local["DBNF"]["global_tau"])
        self.lineage: List[Dict[str, Any]] = []
        self.config = cfg_local

    def check_ecnf_per_merge(self, evidence_items: List[Dict[str, Any]]) -> Dict[str, Any]:
        count = len(evidence_items or [])
        score = evidence_score(evidence_items or [])
        req_count = int(self.config["ECNF"]["m_min"])
        score_threshold = float(self.config["ECNF"]["score_threshold"])
        strong_score_threshold = float(self.config["ECNF"]["strong_score_threshold"])
        required_signals = int(self.config["ECNF"].get("require_distinct_signals", 2))
        distinct = distinct_signal_types(evidence_items)
        pass_flag = False
        reason = ""
        if (count >= req_count and score >= score_threshold and distinct >= required_signals):
            pass_flag = True
            reason = "count_score_and_diversity"
        elif (score >= strong_score_threshold and distinct >= 1):
            pass_flag = True
            reason = "strong_score"
        else:
            pass_flag = False
            reason = "insufficient_evidence"
        return {"pass": pass_flag, "count": count, "score": score, "distinct_signals": distinct, "reason": reason}

    def recompute_canonical(self, alias_embeddings: List[np.ndarray], weights: Optional[List[float]] = None) -> np.ndarray:
        if not alias_embeddings:
            raise ValueError("No alias embeddings provided")
        X = np.stack(alias_embeddings, axis=0)
        if weights is None:
            return np.mean(X, axis=0)
        w = np.asarray(weights).reshape(-1, 1)
        return (w * X).sum(axis=0) / w.sum()

    def check_dbnf_attr(self, pre_vec: Optional[np.ndarray], post_vec: Optional[np.ndarray], tau_attr: float) -> Tuple[bool, float]:
        if pre_vec is None or post_vec is None:
            return True, 0.0
        if np.linalg.norm(pre_vec) > 0:
            pre_vec = pre_vec / (np.linalg.norm(pre_vec) + 1e-12)
        if np.linalg.norm(post_vec) > 0:
            post_vec = post_vec / (np.linalg.norm(post_vec) + 1e-12)
        d = float(np.linalg.norm(pre_vec - post_vec))
        return (d <= tau_attr), d

    def apply_merge(self,
                    base_srs: Dict[str, Any],
                    derived_schema: Dict[str, Any],
                    evidence_map: Dict[str, List[Dict[str, Any]]],
                    embed_fn) -> Dict[str, Any]:
        post_attrs = [dict(a) for a in base_srs.get("attributes", [])]
        name_to_idx = {a["name"]: i for i, a in enumerate(post_attrs)}
        diff = {"added": [], "removed": [], "aliases_added": []}
        to_remove = set()

        for da in derived_schema.get("attributes", []):
            dname = da["name"]
            evidence_items = evidence_map.get(dname, [])
            ecnf_report = self.check_ecnf_per_merge(evidence_items)
            ecnf_pass = ecnf_report["pass"]
            if not ecnf_pass:
                self.record_lineage("merge_pending", dname, None, evidence_items, auto=False, extra={"ecnf": ecnf_report})
                continue

            # choose canonical by highest evidence score among candidate tokens
            candidate_scores = {}
            for ev in evidence_items:
                token = ev.get("token")
                if not token:
                    continue
                candidate_scores.setdefault(token, []).append(ev)
            # compute aggregate score per candidate
            best_candidate = None
            best_score = -1.0
            for cand, evs in candidate_scores.items():
                s = evidence_score(evs)
                if s > best_score:
                    best_score = s
                    best_candidate = cand

            if not best_candidate:
                # fallback: use first evidence token
                best_candidate = evidence_items[0].get("token") if evidence_items else dname

            canonical = best_candidate
            if canonical in name_to_idx:
                idx = name_to_idx[canonical]
                aliases = post_attrs[idx].setdefault("aliases", [])
                if dname not in aliases and dname != canonical:
                    aliases.append(dname)
                    diff["aliases_added"].append((canonical, dname))
                    self.record_lineage("merge", dname, canonical, evidence_items, auto=True, extra={"ecnf": ecnf_report, "agg_score": best_score})
                if dname in name_to_idx and dname != canonical:
                    to_remove.add(dname)
            else:
                new_attr = {
                    "name": canonical,
                    "type": da.get("type", "string"),
                    "aliases": [dname],
                    "provenance": da.get("provenance", {"source": "derived"})
                }
                post_attrs.append(new_attr)
                name_to_idx[canonical] = len(post_attrs) - 1
                diff["added"].append(canonical)
                self.record_lineage("merge_create", dname, canonical, evidence_items, auto=True, extra={"ecnf": ecnf_report, "agg_score": best_score})

        compact = []
        for a in post_attrs:
            if a["name"] in to_remove:
                diff["removed"].append(a["name"])
                continue
            compact.append(a)

        return {"attributes": compact, "diff": diff}

    def execute_merge(self,
                      srs_base: Dict[str, Any],
                      derived_schema: Dict[str, Any],
                      evidence_map: Dict[str, List[Dict[str, Any]]],
                      embed_fn,
                      pre_canonical_embeddings: Optional[Dict[str, np.ndarray]] = None,
                      per_attribute_tau: Optional[Dict[str, float]] = None) -> Dict[str, Any]:
        before = {"attributes": [dict(a) for a in srs_base.get("attributes", [])]}
        after_struct = self.apply_merge(srs_base, derived_schema, evidence_map, embed_fn)

        post_embeddings = {}
        dbnf_details = []
        for a in after_struct["attributes"]:
            name = a["name"]
            tokens = [name] + a.get("aliases", [])
            alias_embs = [embed_fn(t) for t in tokens]
            new_emb = self.recompute_canonical(alias_embs)
            post_embeddings[name] = new_emb
            pre_emb = None
            if pre_canonical_embeddings and name in pre_canonical_embeddings:
                pre_emb = pre_canonical_embeddings[name]
            tau_attr = None
            if per_attribute_tau and name in per_attribute_tau:
                tau_attr = per_attribute_tau[name]
            else:
                tau_attr = self.tau_dbnf
            ok, drift = self.check_dbnf_attr(pre_emb, new_emb, tau_attr)
            action = "accept" if ok else "fork"
            dbnf_details.append({"attribute": name, "drift": float(drift), "pass": bool(ok), "action": action, "tau": float(tau_attr)})
            if not ok:
                # create a new version entry in lineage (simple versioning)
                self.record_lineage("fork", name, None, [], auto=True, extra={"tau": float(tau_attr), "action": "fork"})
        result = {"before": before, "after": after_struct, "dbnf_details": dbnf_details, "lineage": list(self.lineage)}
        return result

    def record_lineage(self, action: str, from_attr: Optional[str], to: Optional[str], evidence: Optional[List[Dict[str, Any]]], auto: bool = True, extra: Optional[Dict[str, Any]] = None):
        entry = {
            "timestamp": datetime.datetime.utcnow().isoformat() + "Z",
            "action": action,
            "from": [from_attr] if from_attr else [],
            "to": to,
            "evidence": evidence or [],
            "actor": "auto" if auto else "pending_human"
        }
        if extra:
            entry.update(extra)
        self.lineage.append(entry)
        logger.debug("Lineage recorded: %s", entry)


# --- Content from C:/research/SemanticNormalForms\srs_store.py ---
# srs_store.py
"""
SRSStore updated to:
- use multi-level embeddings with regenerations for EENF
- build richer per-merge evidence (component-wise, ontology, cooccurrence, source diversity)
- compute per-attribute adaptive DBNF taus and use them in merges
- create simple schema versioning on forks (append new version entries)
- parameterize domain heuristics and thresholds via sdnf_config.json
"""

import datetime
import logging
from typing import Dict, Any, List, Optional, Tuple
import numpy as np
import re
from collections import defaultdict

from embedding_utils import EmbeddingModel
from semantic_merge import SemanticMerger, evidence_score
from validators import SDNFValidator
from sdnf_config import load_config

logger = logging.getLogger("sdnf.srs_store")
logging.basicConfig(level=logging.INFO)

class SRSStore:
    def __init__(self,
                 initial_master: Optional[Dict[str, Any]] = None,
                 embedding_model: Optional[EmbeddingModel] = None,
                 merger: Optional[SemanticMerger] = None,
                 validator: Optional[SDNFValidator] = None):
        self.config = load_config()
        self.model = embedding_model or EmbeddingModel()
        self.merger = merger or SemanticMerger(m_min=self.config["ECNF"]["m_min"], tau_dbnf=self.config["DBNF"]["global_tau"])
        self.validator = validator or SDNFValidator({
            "EENF": self.config["EENF"]["tau"],
            "AANF": self.config["AANF"]["tau"],
            "CMNF": self.config["CMNF"]["tau"],
            "DBNF": self.config["DBNF"]["global_tau"],
            "ECNF": self.config["ECNF"]["m_min"],
            "RRNF": self.config["RRNF"]["tau"],
            "PONF": self.config["PONF"]["tau"]
        })

        if initial_master is None:
            self.master = {"attributes": [], "relations": [], "versions": [{"version": 1, "attributes": []}]}
        else:
            self.master = initial_master
            if "versions" not in self.master:
                self.master["versions"] = [{"version": 1, "attributes": list(self.master.get("attributes", []))}]

        self.records: List[Dict[str, Any]] = []
        self.lineage: List[Dict[str, Any]] = list(self.merger.lineage)
        self._canonical_embeddings: Dict[str, np.ndarray] = {}
        self.attribute_stats: Dict[str, Dict[str, Any]] = {}
        self._recompute_all_canonical_embeddings()

    def _now_iso(self) -> str:
        return datetime.datetime.utcnow().isoformat() + "Z"

    def _embed(self, token: str, context: Optional[str] = None, regen_idx: Optional[int] = None) -> np.ndarray:
        emb = self.model.encode([token], context=context, regen_idx=regen_idx)[0]
        n = np.linalg.norm(emb) + 1e-12
        return (emb / n).astype(np.float32)

    def _recompute_all_canonical_embeddings(self):
        self._canonical_embeddings = {}
        for a in self.master.get("attributes", []):
            name = a["name"]
            tokens = [name] + a.get("aliases", [])
            recent_aliases = self._collect_recent_aliases_for(name, window=1000)
            tokens.extend(recent_aliases)
            emb_list = [self._embed(t) for t in tokens]
            if emb_list:
                centroid = np.mean(np.stack(emb_list, axis=0), axis=0)
                if np.linalg.norm(centroid) > 0:
                    centroid = centroid / (np.linalg.norm(centroid) + 1e-12)
                self._canonical_embeddings[name] = centroid
                stats = self.attribute_stats.setdefault(name, {})
                stats.setdefault("alias_embs", [])
                stats["alias_embs"].extend(emb_list)
                stats["alias_embs"] = stats["alias_embs"][-500:]
                arr = np.stack(stats["alias_embs"], axis=0)
                var = float(np.mean(np.var(arr, axis=0)))
                stats["variance"] = var
                stats["count"] = arr.shape[0]
            else:
                self._canonical_embeddings[name] = self._embed(name)
                self.attribute_stats.setdefault(name, {})["variance"] = 0.0
                self.attribute_stats.setdefault(name, {})["count"] = 0

    def _collect_recent_aliases_for(self, canonical_name: str, window: int = 500) -> List[str]:
        aliases = []
        for rec in reversed(self.records[-window:]):
            mapped = rec.get("mapped_to", {})
            for k, v in mapped.items():
                if v == canonical_name:
                    aliases.append(k)
        seen = set()
        out = []
        for t in aliases:
            if t not in seen:
                seen.add(t)
                out.append(t)
        return out

    def _find_best_match(self, token: str, threshold: float = 0.82) -> Optional[Tuple[str, float]]:
        if not self._canonical_embeddings:
            return None
        q = self._embed(token)
        names = list(self._canonical_embeddings.keys())
        mats = np.stack([self._canonical_embeddings[n] for n in names], axis=0)
        sims = mats @ q
        best_idx = int(np.argmax(sims))
        best_score = float(sims[best_idx])
        if best_score >= threshold:
            return names[best_idx], best_score
        return None

    def _type_of(self, attr_name: str) -> str:
        if not attr_name:
            return "string"
        for a in self.master.get("attributes", []):
            if a.get("name") == attr_name:
                return a.get("type", "string")
        return "string"

    def _ontology_lookup(self, token: str) -> Optional[str]:
        token_l = token.lower()
        mappings = self.config.get("ontology", {}).get("mappings", {})
        for k, v in mappings.items():
            if k in token_l:
                return v
        return None

    def compute_adaptive_dbnf_taus(self) -> Dict[str, float]:
        taus: Dict[str, float] = {}
        cfg = self.config["DBNF"]
        global_tau = float(cfg["global_tau"])
        multiplier = float(cfg.get("adaptive_multiplier", 2.5))
        min_alias_count = int(cfg.get("min_alias_count", 3))
        for name, stats in self.attribute_stats.items():
            var = float(stats.get("variance", 0.0))
            count = int(stats.get("count", 0))
            variability = float(np.sqrt(var)) if var >= 0 else 0.0
            tau_candidate = multiplier * variability
            if count < min_alias_count:
                tau_attr = max(global_tau, 0.9 * tau_candidate)
            else:
                tau_attr = max(global_tau, tau_candidate)
            taus[name] = float(tau_attr)
        for a in self.master.get("attributes", []):
            if a["name"] not in taus:
                taus[a["name"]] = float(global_tau)
        return taus

    def get_master_schema(self) -> Dict[str, Any]:
        return {"attributes": [dict(a) for a in self.master.get("attributes", [])],
                "relations": [dict(r) for r in self.master.get("relations", [])],
                "versions": list(self.master.get("versions", []))}

    def get_records(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        return list(self.records[:limit]) if limit else list(self.records)

    def list_lineage(self, last_n: Optional[int] = 50) -> List[Dict[str, Any]]:
        entries = list(self.merger.lineage)
        return entries[-last_n:] if last_n else entries

    def check_payload_compliance(self, payload: Dict[str, Any], context: Optional[str] = None) -> Dict[str, Any]:
        mapped = {}
        violations = []
        for k, v in payload.items():
            match = self._find_best_match(k, threshold=self.validator.tau.get("AANF", 0.88))
            mapped[k] = match[0] if match else None

        if context:
            for k, attr in mapped.items():
                if attr:
                    attr_meta = next((a for a in self.master.get("attributes", []) if a["name"] == attr), None)
                    if attr_meta:
                        allowed = attr_meta.get("contexts")
                        if allowed and context not in allowed:
                            violations.append({"field": k, "reason": "ContextMismatch", "details": {"expected": allowed, "found": context}})

        attr_names = [a["name"] for a in self.master.get("attributes", [])]
        attr_embeddings = np.stack([self._canonical_embeddings.get(n, self._embed(n)) for n in attr_names], axis=0) if attr_names else np.zeros((0, self.model.dim))
        pre_vec = np.mean(attr_embeddings, axis=0) if attr_embeddings.size else np.zeros((self.model.dim,))
        payload_embs = np.stack([self._embed(k, context=context) for k in payload.keys()], axis=0) if payload else np.zeros((0, self.model.dim))
        post_vec = np.mean(np.vstack([pre_vec, np.mean(payload_embs, axis=0)]), axis=0) if payload_embs.size else pre_vec

        data_context = {
            "regenerations": np.vstack([attr_embeddings, attr_embeddings + 1e-6]) if attr_embeddings.size else np.zeros((1, self.model.dim)),
            "attr_embeddings": attr_embeddings,
            "attr_names": attr_names,
            "W_p": np.eye(self.model.dim),
            "W_r": np.eye(self.model.dim),
            "sample_embeddings": attr_embeddings if attr_embeddings.size else np.zeros((1, self.model.dim)),
            "pre_vec": pre_vec,
            "post_vec": post_vec,
            "tau_dbnf": self.validator.tau.get("DBNF", self.config["DBNF"]["global_tau"]),
            "relations": self.master.get("relations", []),
            "role_compat": {},
            "partition_labels": [0] * len(attr_names)
        }

        sdnf_results = self.validator.run_all(data_context)
        compliant = (len(violations) == 0) and all(r["status"] == "PASS" for r in sdnf_results)
        return {"compliant": compliant, "mapped_fields": mapped, "violations": violations, "sdnf_status": sdnf_results}

    def _build_evidence_for_attr(self, derived_name: str, derived_type: str, derived_vec: np.ndarray, context: Optional[str] = None):
        evidence = []
        best_token, best_score = None, 0.0
        if self._canonical_embeddings:
            keys = list(self._canonical_embeddings.keys())
            mats = np.stack([self._canonical_embeddings[k] for k in keys], axis=0)
            sims = np.dot(mats, derived_vec)
            idx = int(np.argmax(sims))
            best_score = float(sims[idx])
            best_token = keys[idx]
            comp_sim = self.model.component_similarity(self._canonical_embeddings[best_token], derived_vec)
            evidence.append({"type": "nn", "score": float(max(0.0, min(1.0, (best_score + 1) / 2))), "token": best_token})
            evidence.append({"type": "abstract_nn", "score": float(max(0.0, min(1.0, (comp_sim['abstract'] + 1) / 2))), "token": best_token})
            evidence.append({"type": "contextual_nn", "score": float(max(0.0, min(1.0, (comp_sim['contextual'] + 1) / 2))), "token": best_token})

        try:
            ont_root = self._ontology_lookup(derived_name)
            if ont_root:
                evidence.append({"type": "ontology", "score": 0.85, "token": ont_root})
        except Exception:
            pass

        type_score = 1.0 if derived_type and best_token and self._type_of(best_token) == derived_type else 0.0
        evidence.append({"type": "type_match", "score": float(type_score), "token": best_token or derived_name})

        coocc_score = 0.0
        source_set = set()
        if self.records:
            coocc_count = 0
            total_count = 0
            for rec in self.records[-2000:]:
                total_count += 1
                if derived_name in rec.get("payload", {}):
                    mapped = rec.get("mapped_to", {})
                    if best_token and best_token in mapped.values():
                        coocc_count += 1
                        source_set.add(rec.get("source", "unknown"))
            if total_count > 0:
                coocc_score = float(coocc_count) / float(total_count)
                if coocc_score > 0:
                    evidence.append({"type": "cooccurrence", "score": min(1.0, coocc_score), "token": best_token or derived_name, "sources": list(source_set)})

        heur_patterns = self.config.get("heuristics", {}).get("token_patterns", [])
        if any(p in derived_name.lower() for p in heur_patterns):
            evidence.append({"type": "heuristic", "score": 0.6, "token": derived_name})

        pan_regex = self.config.get("heuristics", {}).get("pan_regex")
        if pan_regex and re.match(pan_regex, derived_name):
            evidence.append({"type": "heuristic", "score": 0.85, "token": derived_name})

        if source_set and len(source_set) >= 2:
            evidence.append({"type": "source_diversity", "score": 1.0, "token": derived_name, "sources": list(source_set)})

        return evidence

    def insert_payload(self, payload: Dict[str, Any], source: str = "unknown", context: Optional[str] = None, auto_merge: bool = True) -> Dict[str, Any]:
        compliance = self.check_payload_compliance(payload, context=context)
        mapped = compliance["mapped_fields"]
        derived_schema = {"entity": f"DerivedFrom_{source}", "attributes": [], "derived_at": self._now_iso()}
        for k, v in payload.items():
            derived_schema["attributes"].append({
                "name": k,
                "type": "string",
                "aliases": [],
                "provenance": {"source": source, "first_seen": self._now_iso()}
            })

        evidence_map = {}
        for da in derived_schema.get("attributes", []):
            name = da["name"]
            dtype = da.get("type", "string")
            vec = self._embed(name.lower(), context=context)
            evidence_map[name] = self._build_evidence_for_attr(name, dtype, vec, context=context)

        pre_canonical_embeddings = dict(self._canonical_embeddings)
        per_attribute_tau = self.compute_adaptive_dbnf_taus()

        merge_result = None
        if auto_merge:
            merge_result = self.merger.execute_merge(self.master, derived_schema, evidence_map, embed_fn=lambda t: self._embed(t, context=context), pre_canonical_embeddings=pre_canonical_embeddings, per_attribute_tau=per_attribute_tau)
            if merge_result and merge_result.get("after"):
                # apply after-structure and create new version if forks occurred
                self.master = merge_result["after"]
                # if any fork actions, create a new version snapshot
                forks = [d for d in merge_result.get("dbnf_details", []) if not d.get("pass", True)]
                if forks:
                    new_version_id = max(v.get("version", 1) for v in self.master.get("versions", [{"version": 1}])) + 1
                    self.master.setdefault("versions", []).append({"version": new_version_id, "attributes": [dict(a) for a in self.master.get("attributes", [])], "created_at": self._now_iso()})
                self._recompute_all_canonical_embeddings()
                self.lineage.extend(self.merger.lineage)
        else:
            merge_result = {"after": self.master, "dbnf_details": [], "lineage": []}

        record = {
            "id": len(self.records) + 1,
            "payload": payload,
            "source": source,
            "inserted_at": self._now_iso(),
            "mapped_to": mapped,
            "compliance": compliance,
            "merge_result": merge_result
        }
        self.records.append(record)
        return {"record_id": record["id"], "compliance": compliance, "merge_result": merge_result}

    def export_snapshot(self) -> Dict[str, Any]:
        return {
            "master": self.get_master_schema(),
            "records_count": len(self.records),
            "recent_lineage": self.list_lineage(100),
            "timestamp": self._now_iso()
        }


# --- Content from C:/research/SemanticNormalForms\validators.py ---
# validators.py
"""
SDNF validators

Provides tests for the seven Semantic Data Normal Forms:
EENF, AANF, CMNF, DBNF, ECNF (placeholder), RRNF, PONF.

This file replaces the previous implementation of CMNF with a robust
computation that handles differing projection ranks (W_p and W_r).
"""

from typing import List, Dict, Any
import numpy as np

class SDNFValidator:
    def __init__(self, tau_map: Dict[str, float]):
        """
        tau_map: dict with keys "EENF","AANF","CMNF","DBNF","RRNF","PONF"
        Values are numeric thresholds used by tests.
        """
        self.tau = dict(tau_map)

    # -------------------------
    # EENF: Entity Embedding Normal Form
    # -------------------------
    def test_eenf(self, regenerations: np.ndarray) -> Dict[str, Any]:
        """
        regenerations: array shape (G, d) of repeated embeddings for an entity
        Returns dict with name, requirement, actual, status, details
        """
        if regenerations is None or regenerations.size == 0:
            return {"name": "EENF", "req": f"MeanVar < {self.tau.get('EENF', 0.01)}", "actual": None, "status": "NA", "details": {}}
        per_dim_var = np.var(regenerations, axis=0)
        mean_var = float(np.mean(per_dim_var))
        status = "PASS" if mean_var < float(self.tau.get("EENF", 0.01)) else "FAIL"
        return {"name": "EENF", "req": f"MeanVar < {self.tau.get('EENF', 0.01)}", "actual": mean_var, "status": status, "details": {"per_dim_var_mean": mean_var}}

    # -------------------------
    # AANF: Attribute Alias Normal Form
    # -------------------------
    def test_aanf(self, attr_embeddings: np.ndarray, attr_names: List[str]) -> Dict[str, Any]:
        """
        attr_embeddings: (n, d) array of canonical attribute embeddings
        attr_names: list of attribute names (parallel to attr_embeddings)
        """
        if attr_embeddings is None or attr_embeddings.size == 0:
            return {"name": "AANF", "req": f"Sim < {self.tau.get('AANF', 0.9)}", "actual": None, "status": "NA", "details": {}}
        # compute pairwise cosine similarities (upper triangle)
        X = attr_embeddings
        norms = np.linalg.norm(X, axis=1, keepdims=True) + 1e-12
        Xn = X / norms
        sims = Xn @ Xn.T
        n = sims.shape[0]
        # ignore diagonal
        if n <= 1:
            max_sim = 0.0
        else:
            mask = ~np.eye(n, dtype=bool)
            max_sim = float(np.max(sims[mask]))
        status = "PASS" if max_sim < float(self.tau.get("AANF", 0.9)) else "FAIL"
        return {"name": "AANF", "req": f"Sim < {self.tau.get('AANF', 0.9)}", "actual": max_sim, "status": status, "details": {"candidates": []}}

    # -------------------------
    # CMNF: Context Modulation Normal Form
    # -------------------------
    def test_cmnf(self, W_p: np.ndarray, W_r: np.ndarray, sample_embeddings: np.ndarray) -> Dict[str, Any]:
        """
        Compute contamination between two context projections W_p and W_r.

        Approach:
        - Prefer a subspace-overlap measure using singular values of M = Wp_ctx @ Wr_ctx^T.
          This works when W_p and W_r have different output ranks (k_p != k_r).
        - If SVD fails, fallback to projecting sample embeddings and computing mean absolute
          inner product after trimming to the minimum common projection dimension.

        Inputs:
          W_p: (k_p, d) projection matrix for context p
          W_r: (k_r, d) projection matrix for context r
          sample_embeddings: (n, d) canonical embeddings (used for fallback)
        """
        tau = float(self.tau.get("CMNF", 0.05))
        # Validate inputs
        if W_p is None or W_r is None:
            return {"name": "CMNF", "req": f"AvgInnerProd < {tau}", "actual": None, "status": "NA", "details": {}}

        try:
            # Attempt subspace overlap via singular values of cross-projection matrix
            # If W matrices include contextual-only columns, caller should pass contextual slices.
            # M shape: (k_p, k_r)
            M = np.matmul(W_p, W_r.T)
            sv = np.linalg.svd(M, compute_uv=False)
            if sv.size == 0:
                contamination = 0.0
            else:
                # Normalize singular values to [0,1] by dividing by max singular value (if >0)
                # and take mean absolute singular value as contamination metric.
                max_sv = float(np.max(np.abs(sv))) if sv.size > 0 else 1.0
                if max_sv <= 0:
                    contamination = float(np.mean(np.abs(sv)))
                else:
                    contamination = float(np.mean(np.abs(sv) / max_sv))
            status = "PASS" if contamination < tau else "FAIL"
            return {"name": "CMNF", "req": f"AvgInnerProd < {tau}", "actual": contamination, "status": status, "details": {"sv_count": int(sv.size)}}
        except Exception:
            # Fallback: project sample embeddings and compute mean abs inner product on min common dims
            try:
                if sample_embeddings is None or sample_embeddings.size == 0:
                    return {"name": "CMNF", "req": f"AvgInnerProd < {tau}", "actual": None, "status": "NA", "details": {"fallback": "no_samples"}}
                # Project samples
                Xa = np.matmul(sample_embeddings, W_p.T)  # (n, k_p)
                Xb = np.matmul(sample_embeddings, W_r.T)  # (n, k_r)
                kmin = min(Xa.shape[1], Xb.shape[1])
                if kmin == 0:
                    contamination = 0.0
                else:
                    Xa_trim = Xa[:, :kmin]
                    Xb_trim = Xb[:, :kmin]
                    na = np.linalg.norm(Xa_trim, axis=1, keepdims=True) + 1e-12
                    nb = np.linalg.norm(Xb_trim, axis=1, keepdims=True) + 1e-12
                    Xa_n = Xa_trim / na
                    Xb_n = Xb_trim / nb
                    ips = np.abs(np.sum(Xa_n * Xb_n, axis=1))
                    contamination = float(np.mean(ips))
                status = "PASS" if contamination < tau else "FAIL"
                return {"name": "CMNF", "req": f"AvgInnerProd < {tau}", "actual": contamination, "status": status, "details": {"fallback": "trimmed_projection", "kmin": kmin}}
            except Exception as e:
                return {"name": "CMNF", "req": f"AvgInnerProd < {tau}", "actual": None, "status": "NA", "details": {"error": str(e)}}

    # -------------------------
    # DBNF: Drift Bounded Normal Form (global check)
    # -------------------------
    def test_dbnf(self, pre_vec: np.ndarray, post_vec: np.ndarray) -> Dict[str, Any]:
        tau = float(self.tau.get("DBNF", 0.25))
        if pre_vec is None or post_vec is None:
            return {"name": "DBNF", "req": f"Drift <= {tau}", "actual": None, "status": "NA", "details": {}}
        # normalize and compute L2 distance
        if np.linalg.norm(pre_vec) > 0:
            pre = pre_vec / (np.linalg.norm(pre_vec) + 1e-12)
        else:
            pre = pre_vec
        if np.linalg.norm(post_vec) > 0:
            post = post_vec / (np.linalg.norm(post_vec) + 1e-12)
        else:
            post = post_vec
        drift = float(np.linalg.norm(pre - post))
        status = "PASS" if drift <= tau else "FAIL"
        return {"name": "DBNF", "req": f"Drift <= {tau}", "actual": drift, "status": status, "details": {}}

    # -------------------------
    # ECNF: Evidence Completeness Normal Form (placeholder)
    # -------------------------
    def test_ecnf(self, evidence_summary: Dict[str, Any]) -> Dict[str, Any]:
        """
        Placeholder: the demo computes ECNF per-merge elsewhere.
        This function returns NA unless a numeric summary is provided.
        """
        if not evidence_summary:
            return {"name": "ECNF", "req": "Evidence >= m_min OR agg_score >= threshold", "actual": None, "status": "NA", "details": {}}
        # Expect evidence_summary to contain 'avg_count' and 'avg_score' or similar
        return {"name": "ECNF", "req": "per-merge checks", "actual": evidence_summary, "status": "NA", "details": {}}

    # -------------------------
    # RRNF: Relation Role Normal Form (placeholder)
    # -------------------------
    def test_rrnf(self, relations: List[Dict[str, Any]], role_compat: Dict[str, Any]) -> Dict[str, Any]:
        # If no relations, trivially pass
        if not relations:
            return {"name": "RRNF", "req": f">= {self.tau.get('RRNF', 0.7)}", "actual": 1.0, "status": "PASS", "details": {"total_paths": 0, "incompatible": 0}}
        # Placeholder simple check: count incompatible paths (not implemented)
        return {"name": "RRNF", "req": f">= {self.tau.get('RRNF', 0.7)}", "actual": 1.0, "status": "PASS", "details": {"total_paths": 0, "incompatible": 0}}

    # -------------------------
    # PONF: Partition Orthogonality Normal Form (placeholder)
    # -------------------------
    def test_ponf(self, embeddings: np.ndarray, partition_labels: List[int]) -> Dict[str, Any]:
        if embeddings is None or embeddings.size == 0 or not partition_labels:
            return {"name": "PONF", "req": f"<= {self.tau.get('PONF', 0.1)}", "actual": 0.0, "status": "PASS", "details": {}}
        # Simple implementation: compute mean inter-partition cosine overlap
        labels = np.array(partition_labels)
        unique = np.unique(labels)
        if unique.size <= 1:
            return {"name": "PONF", "req": f"<= {self.tau.get('PONF', 0.1)}", "actual": 0.0, "status": "PASS", "details": {}}
        # compute centroids per partition
        centroids = []
        for u in unique:
            idx = np.where(labels == u)[0]
            if idx.size == 0:
                centroids.append(np.zeros((embeddings.shape[1],)))
            else:
                centroids.append(np.mean(embeddings[idx], axis=0))
        C = np.stack(centroids, axis=0)
        Cn = C / (np.linalg.norm(C, axis=1, keepdims=True) + 1e-12)
        sims = np.abs(Cn @ Cn.T)
        mask = ~np.eye(sims.shape[0], dtype=bool)
        overlap = float(np.mean(sims[mask])) if mask.any() else 0.0
        status = "PASS" if overlap <= float(self.tau.get("PONF", 0.1)) else "FAIL"
        return {"name": "PONF", "req": f"<= {self.tau.get('PONF', 0.1)}", "actual": overlap, "status": status, "details": {}}

    # -------------------------
    # run_all: convenience wrapper
    # -------------------------
    def run_all(self, data_context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        data_context expected keys:
          - regenerations (array Gxd) for EENF
          - attr_embeddings (nxd) and attr_names for AANF
          - W_p, W_r, sample_embeddings for CMNF
          - pre_vec, post_vec for DBNF
          - evidence_summary for ECNF (optional)
          - relations, role_compat for RRNF
          - partition_labels for PONF
        """
        results = []
        # EENF
        regs = data_context.get("regenerations")
        results.append(self.test_eenf(regs))
        # AANF
        attr_emb = data_context.get("attr_embeddings")
        attr_names = data_context.get("attr_names", [])
        results.append(self.test_aanf(attr_emb, attr_names))
        # CMNF
        results.append(self.test_cmnf(data_context.get("W_p"), data_context.get("W_r"), data_context.get("sample_embeddings")))
        # DBNF
        results.append(self.test_dbnf(data_context.get("pre_vec"), data_context.get("post_vec")))
        # ECNF (summary placeholder)
        results.append(self.test_ecnf(data_context.get("evidence_summary")))
        # RRNF
        results.append(self.test_rrnf(data_context.get("relations", []), data_context.get("role_compat", {})))
        # PONF
        results.append(self.test_ponf(data_context.get("attr_embeddings"), data_context.get("partition_labels", [])))
        return results


# --- Content from C:/research/SemanticNormalForms\visualize.py ---
# visualize.py
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import os
import numpy as np

def plot_pre_post(vecs_pre, vecs_post, labels, save_path):
    """Visualizes Contextual Transformation (CMNF) with t-SNE."""
    if not os.path.exists(os.path.dirname(save_path) or "results"):
        os.makedirs(os.path.dirname(save_path) or "results")

    combined = np.vstack([np.asarray(vecs_pre), np.asarray(vecs_post)])
    # fix perplexity relative to sample size
    n = combined.shape[0]
    perplex = min(30, max(5, n//3))
    tsne = TSNE(n_components=2, random_state=42, perplexity=perplex)
    transformed = tsne.fit_transform(combined)
    mid = len(vecs_pre)
    pre_2d = transformed[:mid]
    post_2d = transformed[mid:]

    plt.figure(figsize=(10, 6))
    plt.scatter(pre_2d[:, 0], pre_2d[:, 1], c='blue', label='Original (Generic)', alpha=0.6)
    plt.scatter(post_2d[:, 0], post_2d[:, 1], c='red', label='Projected (Payment Context)', alpha=0.6)

    for i, txt in enumerate(labels[:mid]):
        plt.annotate(txt, (pre_2d[i, 0], pre_2d[i, 1]), fontsize=8)

    plt.title("CMNF Projection: Semantic Context Shift")
    plt.legend()
    plt.savefig(save_path)
    plt.close()


